\documentclass[a4paper,11pt]{article} 
% Load some standard packages
\usepackage{amsmath,parskip,fullpage,natbib} 

% Load tikz for the tree diagram
\usepackage{tikz}
\usetikzlibrary{trees,shapes}

% Define my own commands
\newcommand{\info}{{\cal I}}
\newcommand{\E}{\text{E}}

% Bibliography style
\bibliographystyle{agsm}

\begin{document}  
\title{Inference part}
\author{Ziwen Tang}
\maketitle


\section{Inference for the fixed effect}

As discussed in Section 2,
\begin{equation}\label{eq:4} 
         \widehat{\beta} = (X^T H^{-1} X)^{-1} X^T H^{-1} y
\end{equation}
\begin{equation}\label{eq:7} 
  \begin{split}
    var(\widehat{\beta}) &= (x^T H^{-1} x)^{-1}(x^T H^{-1} var(Y) H^{-1} x)(x^T H^{-1} x)\\
                         &=(x^T H^{-1} x)^{-1}
  \end{split}
\end{equation}
In conducting hypothesis tests, we will use $\beta$ and its estimated asymptotic covariance matrix

\section{2.1 Approximate F-Tests}
In general,for any given contrast matrix L,the two competing hypotheses are
\begin{equation}\label{eq:8} 
         H_0 : L\beta = 0, versus H_A: L\beta \neq 0
\end{equation}
Under the null hypothesis, it can be shown that  the distribution of F-approximation shows
\begin{equation}\label{eq:9}
F = \frac{(\widehat{\beta}-\beta)^T L^T [L（X^T H(\widehat{\theta}) X）^(-1) L^T]^(-1) L(\widehat{\beta}-\beta)}{rank(L)}
\end{equation}
(i doubt that the hypothesis and statistics are matching or not.)
The numerator degrees of freedom equals rank(L).The denominator degrees of freedom needs to be estimated from the data.In this case F has asymptotically a $\frac{1}{d}{\chi_d}^2$distribution(which can be thought of as the limiting distribution of an $F_{d,m}$distribution when  $m \to \infty$ .)

We might try to use the F-test used in standard linear models to perform hypothesis tests regarding the fixed effects.In the standard linear model setting, provided the normality assumption is correct, the null distribution has an exact F-distribution.Unfortunately, problems arise in transferring this method to mixed effect model.Firstly, the definition of degrees of freedom becomes murky in the presence of random effect parameters.Secondly, the test statistic is not necessarily F-distributed.In order to adjust the degrees of freedom, Kenward and Roger(1997)proposed a scaled wald statistic which is illustrated in the chapter 4.2.Even if the adjustment is optimal,there remains the problem that the null distribution may not be F.Furthermore,the method is relevant only for the testing of fixed effects.

\section{2.2 likelihood ratio test}

A classical statistical test for the comparison of nested models with different mean structure is the likelihood ratio(LR)test.It is always possible to exploit that the likelihood ratio test has a limiting $\chi^2$ distribution as the amount of information in the sample goes to infinity.We shall refer to this test as the asymptotic $\chi^2$ test.

Suppose that the null hypothesis of interest is given by $H_0 :\beta \in \theta_{\beta,0}$,for some subspace $\theta_{\beta,0}$ of the parameter space $\theta_\beta$ of the fixed effects $\beta$.

Let $L_ML$ denote again the ML likelihood function and let $-2ln\lambda_N$ be the likelihood ratio test statistic defined as
\begin{equation}\label{eq:10}
-2ln\lambda_N = -2 ln[\frac{L_{ML}(\widehat{\theta}_{ML,0})}{L_{ML}(\widehat{\theta}_{ML})}]
\end{equation}
where $\widehat{\theta}_{ML,0}$ and $\widehat{\theta}_{ML}$ are the maximum likelihood estimates obtained from maximizing $L_ML$ over $\theta_{\beta,0}$ and $\theta_\beta$,respectively.It then follows from classical likelihood theory(see,e.g.,Cox and Hinkley 1990,Chapter 9)that, under some regularity conditions,$-2ln \lambda_N$ follows,asymptotically under $H_0$,a chi-squared distribution with degrees of freedom equal to the difference between the dimension p of $\theta_\beta$ and the dimension of $\theta_{\beta,0}$.

(the test statistics is approximately chi-squared with degrees of freedom equal to the difference in the dimensions of the two parameters spaces(the difference in the number of parameters when the models are identifiable.)

Why not reml?

It should be emphasized that the above result is not valid if the models are fitted using REML rather than ML estimation.Indeed, the mean structure of the model fitted under $H_0$ is not the mean structure $X\beta$ of the original model under $\theta_\beta$,leading to different error contrasts$w = A^T y$ under both models.Hence,the corresponding REML log-likelihood functions are based on different observations,which makes them no longer comparable.

The reason is that REML estimates the random effects by considering linear combinations of the data the remove the fixed effects.If these fixed effects are changed,the likelihoods of the two models will not be directly comparable.
\section{4.2 Kenward and Roger}

An adjusted covariance estimate which accounts for the extra variability introduced by estimating $\theta$,and they show that its small sample distribution can be well approximated by an F-distribution with denominator degrees of freedom also obtained via a Satterthwaite-type approximation.Kenward and Roger(1997)implemented a Satterthwaite-based approximation for the following scaled F-statistic:
This procedure allows the covariance among the Reml covariance parameter estimates to be taken into account when estimationg the effective degrees of freedom of the F-test and thus different contrasts will exhibit different degrees of freedom.details on the computation ofk,cov,and the efective degrees of freedom can be found in (kenward and roger,1997)

[reference]

statistical analysis of longitudinal neuroimage data with linear mixed effects models
\end{document} 


\documentclass[a4paper,11pt]{article} 
% Load some standard packages
\usepackage{amsmath,parskip,fullpage,natbib} 

% Load tikz for the tree diagram
\usepackage{tikz}
\usetikzlibrary{trees,shapes}

% Define my own commands
\newcommand{\info}{{\cal I}}
\newcommand{\E}{\text{E}}

% Bibliography style
\bibliographystyle{agsm}

\begin{document}  
\title{Inference part}
\author{Ziwen Tang}
\maketitle


\section{Inference for the fixed effect}

As discussed in Section 2,
\begin{equation}\label{eq:4} 
         \widehat{\beta} = (X^T H^{-1} X)^{-1} X^T H^{-1} y
\end{equation}
\begin{equation}\label{eq:7} 
  \begin{split}
    var(\widehat{\beta}) &= (x^T H^{-1} x)^{-1}(x^T H^{-1} var(Y) H^{-1} x)(x^T H^{-1} x)\\
                         &=(x^T H^{-1} x)^{-1}
  \end{split}
\end{equation}
In conducting hypothesis tests, we will use $\beta$ and its estimated asymptotic covariance matrix
\section{2.1 Approximate Wald Tests}
In general,for any known matrix L,a test for the hypothesis
\begin{equation}\label{eq:8} 
         H_0 : L\beta = 0, versus H_A: L\beta \neq 0
\end{equation}
immediately follows from the fact that the distribution of 
\begin{equation}\label{eq:8}
(\widehat{\beta}-\beta)^T L^T [L（X^T H X）^(-1) L^T]^(-1) L(\widehat{\beta}-\beta)
\end{equation}
asymptotically follows a chi-squared distribution with rank(L) degrees of freedom.
Disadvantages:As noted by Dempster,Rubin and Tsutakawa(1981),the Wald test statistics are based on estimated standard errors which underestimate the true variablility in $\widehat{\beta}$ because they do not take into account the variability introduced by estimating $\theta$.In practice,this downward bias is often resolved by using approximate t- and F-statistics for testing hypotheses about $\beta$
\section{2.2 Approximate t-Tests and F-Tests}
In general,for any known matrix L,a test for the hypothesis
\begin{equation}\label{eq:8} 
         H_0 : L\beta = 0, versus H_A: L\beta \neq 0
\end{equation}
is based on an F-approximation to the distribution of
\begin{equation}\label{eq:9}
F = \frac{(\widehat{\beta}-\beta)^T L^T [L（X^T H(\widehat{\theta}) X）^(-1) L^T]^(-1) L(\widehat{\beta}-\beta)}{rank(L)}
\end{equation}
The numerator degrees of freedom equals rank(L).The denominator degrees of freedom needs to be estimated from the data.The same is true for the degrees of freedom needed in the above t-approximation. We might try to use the F-test used in standard linear models to perform hypothesis tests regarding the fixed effects.In the standard linear model setting, provided the normality assumption is correct, the null distribution has an exact F-distribution.Unfortunately, problems arise in transferring this method to mixed effect model.Firstly, the definition of degrees of freedom becomes murky in the presence of random effect parameters.Secondly, the test statistic is not necessarily F-distributed.In order to adjust the degrees of freedom, Kenward and Roger(1997)proposed a scaled wald statistic which is illustrated in the chapter 4.2.Even if the adjustment is optimal,there remains the problem that the null distribution may not be F.Furthermore,the method is relevant only for the testing of fixed effects.
\section{2.3 likelihood ratio test}
A classical statistical test for the comparison of nested models with different mean structure is the likelihood ratio(LR)test.
Suppose that the null hypothesis of interest is given by $H_0 :\beta \in \theta_{\beta,0}$,for some subspace $\theta_{\beta,0}$ of the parameter space $\theta_\beta$ of the fixed effects $\beta$.
Let $L_ML$ denote again the ML likelihood function and let $-2ln\lambda_N$ be the likelihood ratio test statistic defined as
\begin{equation}\label{eq:10}
-2ln\lambda_N = -2 ln[\frac{L_{ML}(\widehat{\theta}_{ML,0})}{L_{ML}(\widehat{\theta}_{ML})}]
\end{equation}
where $\widehat{\theta}_{ML,0}$ and $\widehat{\theta}_{ML}$ are the maximum likelihood estimates obtained from maximizing $L_ML$ over $\theta_{\beta,0}$ and $\theta_\beta$,respectively.It then follows from classical likelihood theory(see,e.g.,Cox and Hinkley 1990,Chapter 9)that, under some regularity conditions,$-2ln \lambda_N$ follows,asymptotically under $H_0$,a chi-squared distribution with degrees of freedom equal to the difference between the dimension p of $\theta_\beta$ and the dimension of $\theta_{\beta,0}$.
(the test statistics is approximately chi-squared with degrees of freedom equal to the difference in the dimensions of the two parameters spaces(the difference in the number of parameters when the models are identifiable.)
Why not reml?
It should be emphasized that the above result is not valid if the models are fitted using REML rather than ML estimation.Indeed, the mean structure of the model fitted under $H_0$ is not the mean structure $X\beta$ of the original model under $\theta_\beta$,leading to different error contrasts$w = A^T y$ under both models.Hence,the corresponding REML log-likelihood functions are based on different observations,which makes them no longer comparable.
The reason is that REML estimates the random effects by considering linear combinations of the data the remove the fixed effects.If these fixed effects are changed,the likelihoods of the two models will not be directly comparable.
\section{4.2 Kenward and Roger}
An adjusted covariance estimate which accounts for the extra variability introduced by estimating $\theta$,and they show that its small sample distribution can be well approximated by an F-distribution with denominator degrees of freedom also obtained via a Satterthwaite-type approximation.
\end{document} 


